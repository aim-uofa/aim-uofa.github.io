<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>StaMo</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon2.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          
          <!-- <img src="1.jpg" alt="" style="width: 50%; height: auto;"> -->
          <!-- <h1 class="title is-1 publication-title">
            <img src="1.jpg" alt="" style="height: 1em; vertical-align: middle;">
            StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation
          </h1> -->
          <h1 class="title is-1 publication-title">	
            <img src="./static/images/icon2.png" alt="" style="width: 7%; height: auto;">
            StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mingyulau.github.io">Mingyu Liu*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/hello3x3">Jiuhe Shu*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://oliverchhhh.github.io">Hui Chen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://zeju-li.com">Zeju Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://volcverse.vercel.app/">Canyu Zhao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://yangjiangeyjg.github.io/">Jiange Yang</a><sup>2</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://github.com/Little-Podi">Shenyuan Gao</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://stan-haochen.github.io">Hao Chen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://cshen.github.io/">Chunhua Shen</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Zhejiang University,</span>
            <span class="author-block"><sup>2</sup>Nanjing University,</span>
            <span class="author-block"><sup>3</sup>Hong Kong University of Science and Technology</span>
          </div>

          <span class="author-block"><sup>*</sup>Equal Contribution</span>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2510.05057"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2510.05057"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/aim-uofa/StaMo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            A fundamental challenge in embodied intelligence is developing expressive and compact state representations 
            for efficient world modeling and decision making. However, existing methods often fail to achieve this 
            balance, yielding representations that are either overly redundant or lacking in task-critical information.
             We propose an unsupervised approach that learns a highly compressed two-token state representation using a
              lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong 
              generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing
               VLA-based models, improving performance by <b>14.3%</b> on LIBERO and <b>30%</b> in real-world task success with
               minimal inference overhead. More importantly, we find that the difference between these tokens, obtained
                via latent interpolation, naturally serves as a highly effective latent action, which can be further 
                decoded into executable robot actions. This emergent capability reveals that our representation 
                captures structured dynamics without explicit supervision. We name our method StaMo for its ability 
                to learn generalizable robotic Motion from compact State representation, which is encoded from static 
                images, challenging the prevalent dependence to learning latent action on complex architectures and 
                video data. The resulting latent actions also enhance policy co-training, outperforming prior methods 
                by <b>10.4%</b> with improved interpretability. Moreover, our approach scales effectively across diverse data 
                sources, including real-world robot data, simulation, and human egocentric video.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


  </div>
</section>


<section class="section" id="model">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method</h2>
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <img src="./static/images/StaMo_teaser.png" alt="Omni-R1 Model Architecture"
            style="width: 100%; max-width: 1200px;" />
          <p class="has-text-grey">
            <b>An overview of our StaMo framework.</b>Our method efficiently compresses and encodes robotic visual representations, enabling the learning of a compact state representation. Motion naturally emerges as the difference between these states in the highly compressed token space. This approach facilitates efficient world modeling and demonstrates strong generalization, with the potential to scale up with more data.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="model">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Visualize Results</h2>
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <img src="./static/images/visualize.png" alt="Omni-R1 Model Architecture"
            style="width: 100%; max-width: 1200px;" />
          <p class="has-text-grey">
            <b>Reconstruction, Motion Interpolation and Motion Transfer.</b>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="model">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Interpolation</h2>
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <img src="./static/images/interpolation-1.png" alt="Omni-R1 Model Architecture"
            style="width: 100%; max-width: 1200px;" />
          <p class="has-text-grey">
            <b>Visual reconstruction results from the same episode..</b>The first and last frames are reconstructed from ground-truth images using the StaMo encoder, while the intermediate frames are generated by linearly interpolating between the latent state tokens of the two endpoints. The transitions show that both the robotic arm and the objects move in a continuous and smooth manner.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="model">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Motion Transfer</h2>
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <img src="./static/images/appen-1.png" alt="Omni-R1 Model Architecture"
            style="width: 100%; max-width: 1200px;" />
          <p class="has-text-grey">
            <b>Transfer linear interpolation experiment with the StaMo encoder.</b>The left and right panels illustrate different task scenarios, where reconstructions are obtained by tokens(3) + tokens(2) – tokens(1), demonstrating the linear interpolation property of latent representations during transfer.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="model">
  <h2 class="title is-3 has-text-centered">Goal Condition Policy</h2>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/new2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/new4.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{liu2025stamo,
  title={StaMo: Unsupervised Learning of Generalizable Robotic Motions from Static Images},
  author={Liu, Mingyu and Shu, Jiuhe and Chen, Hui and Li, Zeju and Zhao, Canyu and Yang, Jiange and Gao, Shenyuan and Chen, Hao and Shen, Chunhua},
  journal={arXiv preprint arXiv:2510.05057},
  year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container" style="text-align: left;">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website content is licensed under a <a rel="license"
                                                href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative
            Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page and <a href="https://doubiiu.github.io/projects/ToonCrafter"> ToonCrafter</a> project page. If you want to reuse their <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
</body>
</html>
