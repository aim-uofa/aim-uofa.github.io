<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="generating comics"/>
    <title>AutoStory: Generating Diverse Storytelling Images with Minimal Human Effort</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=DM Mono' rel='stylesheet'>
    <link href="../css/cs.css" rel="stylesheet">
    <style>
      body {
        background: #fdfcf9 no-repeat fixed top left;
        font-family:'DM Mono','Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;"> AutoStory: Generating Diverse Storytelling Images with Minimal Human Effort</h2>
            <h4 style="color:#6e6e6e;"> Ongoing </h4>
            <!-- <h5 style="color:#6e6e6e;"> xxxx </h5> -->
            <hr>

<!--

            <h6> 

              <a href="https://github.com/encounter1997" target="_blank">Wen Wang</a><sup>1</sup>, 
              <a href="https://github.com/DummyNodeHead" target="_blank">Canyu Zhao</a><sup>1</sup>, 
              <a href="https://stan-haochen.github.io/" target="_blank">Hao Chen</a><sup>1</sup>,  
              <a href="https://cshen.github.io/" target="_blank">Chunhua Shen</a><sup>1</sup>
            </h6>
-->



            <p>  
              Zhejiang University
              <br>
           <!--   First two authors contributed equally. -->
            </p>

            
            <!-- 
                <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> 
            -->


            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="#" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="https://github.com/aim-uofa/STORY" role="button" 
                    target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
          <hr style="margin-top:20px">

          <h6 style="color:#8899a5"> Our method is able to generate comic stories in various styles with minimum human interaction. 
            subject identity and story coherence are well preserved without sophisticated training involved.</h6>
          <img class="img-fluid" src="images/pipeline.png" alt="pipeline" width="1024">
          <p style="margin-top: 30px;" class="text-justify">
            Story visualization aims to generate a series of images that match the story
            described in texts, and it requires the generated images to satisfy high quality,
            alignment with the text description, and consistency in character identities.
            Given the complexity of story visualization, existing methods drastically
            simplify the problem by considering only a few specific characters and
            scenarios, or requiring the users to provide per-image control conditions
            such as sketches. However, these simplifications also make it difficult to
            put these methods into application. To this end, this paper proposes an
            automated story generation system that effectively generates diverse, high-
            quality, and consistent sets of story images, with minimal human efforts.
            Technically, we utilize the comprehension and planning capabilities of large
            language models for layout planning, and then leverage large-scale text-to-
            image models to generate complex story images based on the layout. We
            empirically found that sparse control conditions, like bounding boxes, are
            suitable for layout planning, while dense control conditions, like sketch
            and pose, are suitable for generating high-quality image content. In order
            to obtain the best of both worlds, we propose protagonist generation as
            the bridge to transform simple bounding box layouts into dense control
            conditions suitable for image generation. In addition, we propose a simple
            yet effective method to generate multi-view consistent character images,
            eliminating the reliance on human labor to collect or draw character images.
            This allows our method to obtain consistent story visualization even when
            only texts are provided as input.
          </p>

          <h3 style="margin-top:60px">Examples</h3>
            <!-- <h6 style="color:#8899a5"> FrozenRecon can reconstruct 3D scene shape from only a pose-free monocular video in <b style="color:#e94a00">real-time</b>üî•.</h6> -->

<!--             <video poster="images/header-vid-poster.png" width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" id="header_vid"> -->
            <!-- <video width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" id="header_vid">
                  <source src="videos/web-scene2.m4v" type="video/mp4">
            </video> -->

            <hr style="margin-top:20px">
            <h4 style="margin-top: 50px; margin-bottom:50px">Summarize the story about Jack and Rose in Titanic.</h4>
            <img class="img-fluid" src="images/titanic/1.jpeg" alt="demo" width="768">
            <img class="img-fluid" src="images/titanic/2.jpeg" alt="demo" width="768">
            <img class="img-fluid" src="images/titanic/3.jpeg" alt="demo" width="768">
            <img class="img-fluid" src="images/titanic/4.jpeg" alt="demo" width="768">
            <img class="img-fluid" src="images/titanic/5.jpeg" alt="demo" width="768">
            <img class="img-fluid" src="images/titanic/6.jpeg" alt="demo" width="768">

            <hr style="margin-top:20px">
            <h4 style="margin-top: 50px; margin-bottom:50px">Write a short story about 2 girls, whose names are Chisato and Fujiwara.</h4>
            <img class="img-fluid" src="images/2girls/1.jpeg" alt="demo" width="768">
            <img class="img-fluid" src="images/2girls/new2.jpg" alt="demo" width="768">
            <img class="img-fluid" src="images/2girls/new3.jpg" alt="demo" width="768">
            <img class="img-fluid" src="images/2girls/new4.jpg" alt="demo" width="768">

            <hr style="margin-top:20px">
            <h4 style="margin-top: 50px; margin-bottom:50px">Write a short story about two men. Gigachad and Keanu.</h4>
            <img class="img-fluid" src="images/3men/new1.jpg" alt="demo" width="768">
            <img class="img-fluid" src="images/3men/new2.jpg" alt="demo" width="768">
            <img class="img-fluid" src="images/3men/new3.jpg" alt="demo" width="768">

            <hr style="margin-top:20px">
            <h4 style="margin-top: 50px; margin-bottom:50px">Write a short story about 3 girls, Mazaki, Chisato and Hayasaka.</h4>
            <img class="img-fluid" src="images/3girls/1.jpg" alt="demo" width="768">
            <img class="img-fluid" src="images/3girls/2.jpg" alt="demo" width="768">
            <img class="img-fluid" src="images/3girls/3.jpg" alt="demo" width="768">
            <img class="img-fluid" src="images/3girls/4.jpg" alt="demo" width="768">

            <hr style="margin-top:20px">
            <h4 style="margin-top: 50px; margin-bottom:50px">Write a short story about a dog and a cat.</h4>
            <img class="img-fluid" src="images/dog_and_cat/1.jpg" alt="demo" width="768">
            <img class="img-fluid" src="images/dog_and_cat/2.jpg" alt="demo" width="768">
            <img class="img-fluid" src="images/dog_and_cat/3.jpg" alt="demo" width="768">

            <hr style="margin-top:20px">
            <h4 style="margin-top: 50px; margin-bottom:50px">Write a short story about a fox and a cat.</h4>
            <img class="img-fluid" src="images/fox_and_cat/1.jpg" alt="demo" width="768">
            <img class="img-fluid" src="images/fox_and_cat/2.jpg" alt="demo" width="768">
            
            <hr style="margin-top:20px">
            <h4 style="margin-top: 50px">Write a short story about a cat and a bird.</h4>
            <h6 style="color:#8899a5; margin-bottom:50px"> Text input only. Subject description: "a black cat, yellow eyes", "a white bird, red beak"</h6>
            <img class="img-fluid" src="images/cat_and_bird/1.jpg" alt="demo" width="768">

            <hr style="margin-top:20px">
            <h4 style="margin-top: 50px; margin-bottom:50px">Compare with existing story generation methods.</h4>
            <img class="img-fluid" src="images/compare.jpg" alt="demo" width="1500">

            <!-- <img class="img-fluid" src="images/3men/4.jpg" alt="demo" width="768"> -->

            <div><b style="color:#fd5638; font-size:large" id="demo-warning"></b>
            <br>
            </div>
              <!-- <br><br> -->
          <!-- <p class="text-justify">
            3D scene reconstruction is a long-standing vision task. Existing approaches can be categorized into geometry-based and learning-based methods. The former leverages multi-view geometry but can face catastrophic failures due to the reliance on accurate pixel correspondence across views. The latter was proffered to mitigate these issues by learning 2D or 3D representation directly. However, without a large-scale video or 3D training data, it can hardly generalize to diverse real-world scenarios due to the presence of tens of millions or even billions of optimization parameters in the deep network. Recently, robust monocular depth estimation models trained with large-scale datasets have been proven to possess weak 3D geometry prior, but they are insufficient for reconstruction due to the unknown camera parameters, the affine-invariant property, and inter-frame inconsistency. Here, we propose a novel test-time optimization approach that can transfer the robustness of affine-invariant depth models such as LeReS to challenging diverse scenes while ensuring inter-frame consistency, with only dozens of parameters to optimize per video frame. Specifically, our approach involves freezing the pre-trained affine-invariant depth model's depth predictions, rectifying them by optimizing the unknown scale-shift values with a geometric consistency alignment module, and employing the resulting scale-consistent depth maps to robustly obtain camera poses and achieve dense scene reconstruction, even in low-texture regions. Experiments show that our method achieves state-of-the-art cross-dataset reconstruction on five zero-shot testing datasets. 
          </p> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- reconstruction showcase -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Reconstruction showcase</h3>
            <hr style="margin-top:0px">
            <div class="embed-responsive embed-responsive-16by9">

                <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://sketchfab.com/playlists/embed?autostart=1&autospin=0.25&amp;collection=bd885db8684e4876976bd6616eda7ade" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture; fullscreen" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
            </div>
            <br>
            <p> Zoom in by scrolling. You can toggle the ‚ÄúSingle Sided‚Äù option in Model Inspector (pressing I key) to enable back-face culling (see through walls). Select ‚ÄúMatcap‚Äù to inspect the geometry without textures.</p>
        </div>
      </div>
    </div>
  </section>
  <br> -->


  <!-- real-time incremental reconstruction -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Real-time incremental reconstruction</h3>
            <p class="text-justify"> 
              Data is captured around the working area with an iPhone, and the camera poses are obtained from <a href="https://developer.apple.com/documentation/arkit">ARKit</a>.
              The model used here is only trained on ScanNet, which indicates that NeuralRecon generalizes well to new domains.
              The gradual refinement on the reconstruction quality over time (through GRU-Fusion) can also be observed.
            </p>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="videos/web-scene1.m4v" type="video/mp4">
            </video>
            <div class="embed-responsive embed-responsive-16by9">
                <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://sketchfab.com/playlists/embed?autostart=1&autospin=0.25&amp;collection=3d55b6141e18492790509fc93fa453c9" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture; fullscreen" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
            </div>
        </div>
      </div>
    </div>
  </section>
  <br> -->

  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
            <h3>AR demo 1</h3>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" controls autoplay loop="loop" preload="" muted="">
                <source src="videos/ar-1.mp4" type="video/mp4">
            </video>
        </div>
        <div class="col text-center">
            <h3>AR demo 2</h3>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" controls autoplay loop="loop" preload="" muted="">
                <source src="videos/ar-2.mp4" type="video/mp4">
            </video>
        </div>
      </div>
    </div>
  </section>
  <br> -->

  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Generalization to the outdoor scene</h3>
            <p class="text-justify"> 
              The pretrained model of NeuralRecon can generalize reasonably well to outdoor scenes, which are completely out of the domain of the training dataset ScanNet.
            </p>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" controls preload="" muted="">
                <source src="videos/outdoor-midres-8.mp4" type="video/mp4">
            </video>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Handling scenes with extremely low texture</h3>
            <p class="text-justify"> 
              NeuralRecon can handle homogeneous textures (e.g. white walls and tables), thanks to the learned surface priors.
            </p>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" controls  loop="loop" preload="" muted="">
                <source src="videos/textureless-midres-8.mp4" type="video/mp4">
            </video>
        </div>
      </div>
    </div>
  </section>
  <br> -->




  <!-- overview video -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Overview video (5 min)</h3>
            <hr style="margin-top:0px">
            <div class="embed-responsive embed-responsive-16by9">
                <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/wuMPaUTJuO0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
            </div>
        </div>
      </div>
    </div>
  </section>
  <br> -->


  <!-- Pipeline overview -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Pipeline overview</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/frozenrecon-pipeline.png" alt="FrozenRecon Pipeline">
            <hr style="margin-top:0px">
            <p class="text-justify">
              Given a monocular video, we use a frozen robust monocular depth estimation model to obtain the estimated depths of all frames. 
              Then, we propose a geometric consistency alignment module, which optimizes a sparse set of parameters (i.e. scale, shift, and weight factors) to achieve multi-view geometric consistent depths among all frames. 
              The camera's intrinsic parameters and poses are also optimized simultaneously. Finally, we can achieve high-quality dense 3D reconstruction with optimized depths and camera parameters. 
            </p>
        </div>
      </div>
    </div>
  </section>
  <br> -->

  <!-- Comparison with state-of-the-art methods -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with state-of-the-art methods</h3>
            <p class="text-left"> 
             Only the inference time on key frames is computed. Back-face culling is enabled during rendering. Ground-truth is captured using the LiDAR sensor on iPad Pro.</p>
            <hr style="margin-top:0px">
            <p class="text-left" style="color:#646464"> B5-Scene 1:</p>
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls>
                <source src="videos/Comparison1-near.m4v" type="video/mp4">
            </video>
            <p class="text-left" style="color:#646464"> B5-Scene 2:</p>
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls>
              <source src="videos/Comparison2-near.m4v" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </section>
  <br> -->

  <!-- TODO -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>TODO</h3>
            <hr style="margin-top:0px">
            <p class="text-justify">
            1. Reconstruction showcase. 2. Videos.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br> -->


  <!-- citing -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@inproceedings{xu2023frozenrecon,
  title={Pose-free 3D Scene Reconstruction with Frozen Depth Models},
  author={Xu, Guangkai and Yin, Wei and Chen, Hao and Shen, Chunhua and Cheng, Kai and Zhao, Feng},
  journal={ICCV},
  year={2023}
}</code></pre>
          <hr>
      </div>
    </div>
  </div> -->

  <!-- ack -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgements</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            We would like to specially thank Reviewer 3 for the insightful and constructive comments.
            We would like to thank Sida Peng , Siyu Zhang and Qi Fang for the proof-reading.
          </p>
      </div>
    </div>
  </div> -->

  <!-- rec -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Recommendations to other works from our group</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            Welcome to checkout our other works on monocular depth estimation (<a href="https://github.com/aim-uofa/AdelaiDepth">AdelaiDepth</a>). Our work <a href="https://github.com/YvanYin/Metric3D">Metric3D</a> achieves the challenge winner of <a href="https://jspenmar.github.io/MDEC/">2nd Monocular Depth Estimation Challenge Workshop.</a>
          </p>
      </div>
    </div>
  </div> -->



  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
    <!--  Thanks to <a href="https://zju3dv.github.io/neuralrecon/" target="_blank">NeuralRecon</a> for the <a href="https://github.com/zju3dv/zju3dv.github.io/tree/master/neuralrecon/" target="_blank">website template</a>.
    -->
  </footer>

  <script type="text/javascript">
    function changePlaybackSpeed(speed)
        {
            document.getElementById('inspect_vid').playbackRate = speed;
        }
        // changePlaybackSpeed(0.25)

    var demo = document.getElementById("header_vid");
    var startTime;
    var timeout = undefined;
    demo.addEventListener("loadstart", function() {
      startTime = Date.now();
      timeout = setTimeout(function () {
        var demoWarning = document.getElementById("demo-warning");
        var giteeLink = document.createElement("a");
        //giteeLink.innerText = "mirror hosted in mainland China";
        //giteeLink.href = "https://project-pages-1255496016.cos-website.ap-shanghai.myqcloud.com/neuralrecon/";
        // var bilibiliLink = document.createElement("a");
        // var youtubeLink = document.createElement("a");
        // bilibiliLink.innerText = "BiliBili";
        // bilibiliLink.href = "";
        // youtubeLink.innerText = "YouTube";
        // youtubeLink.href = "";

        demoWarning.append("Loading the videos took too long, you can optionally visit this site in the ", giteeLink, ".");
        // demoWarning.append("Loading the video took too long, you can optionally watch it on Bilibili", bilibiliLink, " or YouTube", youtubeLink, ".");
        clearTimeout(timeout);
        timeout = undefined;
      }, 6000);
    });
    demo.addEventListener("loadeddata", function() {
      if (timeout) {
        clearTimeout(timeout);
        timeout = undefined;
      }
    });
//     var source = document.createElement("source");
//     source.setAttribute("src", "/videos/web-scene2.m4v");
//     source.setAttribute("type", "video/webm");
//     demo.appendChild(source);
  </script>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
